version: "3.8"

name: whisperai-project

services:
  whisper:
    container_name: whisperai_container
    build: .
    ports:
      - "5001:5000"
    environment:
      # Set initial Whisper model - options: tiny, base, small, medium, large
      - WHISPER_MODEL=base
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    # These are the key settings for GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      # Optional: For development, uncomment to mount the app.py file
      # - ./app.py:/app/app.py
      # For persistence of models (avoids redownloading)
      - whisper-models:/root/.cache/whisper

volumes:
  whisper-models: # Persistent volume for model storage